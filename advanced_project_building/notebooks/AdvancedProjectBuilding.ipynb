{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced project building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext lab_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import getpass\n",
    "import glob\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a handy reference to the data location for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDirectory = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the Panoptes Python Client \n",
    "The Panoptes Python Client allows you to manage your project using Python scripts. This can be very useful if you need to add large subject sets, request data exports or modify the properties of your subjects. It also allows you to set up advanced subject-specific training and feedback behaviour.\n",
    "\n",
    "You may need to install the `panoptes_client` and `pandas` packages. If you do, then run the code in the next cell. Otherwise, skip it."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!python -m pip install panoptes_client\n",
    "!python -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panoptes_client import Panoptes, SubjectSet, Subject, Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Authenticate with the Panoptes API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "username = input(\"Enter Panoptes Username:\")\n",
    "password = getpass.getpass(\"Enter Panoptes Password:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "panopteClient = Panoptes.connect(username=username, password=password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find our project\n",
    "Enter your project's ID in the field below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projectId = ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `find` method on the `Project` class, passing our project's ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project.find(projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check we found the right project and display some details about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Name:\", project.display_name)\n",
    "print(\"Description:\", project.description)\n",
    "print(\"Introduction:\", project.introduction)\n",
    "print(\"Number of Subjects:\", project.subjects_count)\n",
    "print(\"Number of Classifications:\", project.classifications_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's create a new subject set using the Python API\n",
    "Start by instantiating a new `SubjectSet` object and giving it a `display_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectSet = SubjectSet()\n",
    "subjectSet.display_name = \"Python Demo Set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must also link this subject set with our project. We do so like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectSet.links.project = project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, our subject set only exists in our computer's memory. To actually add it to our project, we must use the `save` method. When we save our new subject set the Panoptes API, sends us a response to let us know that the save succeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = subjectSet.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've successfully created a new subject set, but it's empty! We need to add some subjects to it!\n",
    "\n",
    "### Creating new subjects\n",
    "Subjects are created in Python as instances of the `Subject` class, for example\n",
    "```python\n",
    "newSubject = Subject()\n",
    "```\n",
    "Once we have created an empty `Subject`, we need to link it with our `Project`, like this:\n",
    "```python\n",
    "newSubject.links.project = project\n",
    "```\n",
    "Now we can add one or more images (or other media types) to it using its `add_location` method.\n",
    "```python\n",
    "newSubject.add_location([\"image1.jpg\", \"image2.jpg\"])\n",
    "```\n",
    "We can also use the Python client to add metadata to our `Subject` by assigning a Python `dict` to its `metadata` attribute like this\n",
    "```python\n",
    "newSubjectMetadata = { \"Number of Images\" : 2 }\n",
    "newSubject.metadata = newSubjectMetadata\n",
    "```\n",
    "Just like `SubjectSet`s, subjects must also be `save`d.\n",
    "```python\n",
    "newSubject.save()\n",
    "```\n",
    "Finally, we should add our new subject to our `SubjectSet`.\n",
    "```python\n",
    "subjectSet.add(newSubject)\n",
    "```\n",
    "Now let's try this out for real! We'll use the images in our second demo test set (`demoset_10-20`). Let's get a list of those files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imageFiles = glob.glob(os.path.join(dataDirectory, \"demoset_10-20/*jpg\"))\n",
    "imageFiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use those files to create a list of new `Subjects` and add some metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newSubjects = []\n",
    "\n",
    "for imageFile in imageFiles:\n",
    "\n",
    "    newSubject = Subject()\n",
    "    newSubject.links.project = project\n",
    "    newSubject.add_location(imageFile)\n",
    "    newSubject.metadata = {\n",
    "        \"Origin\": \"Python demo\",\n",
    "        \"image\": os.path.basename(imageFile),\n",
    "        \"#Local File Path\": imageFile,\n",
    "    }\n",
    "    newSubject.save()\n",
    "\n",
    "    newSubjects.append(newSubject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign our newly uploaded `Subject`s to the `SubjectSet` we already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectSet.add(newSubjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving the volunteer experience\n",
    "Our volunteers are an enthusiastic bunch so we decide to reward them by providing some links to extra information about the subjects they're classifying. We'll add these links to the existing subjects as metadata.\n",
    "\n",
    "**But**, we don't want to influence our volunteers' decisions, so we'll add metadata that can only be seen from the _Talk_ interface, **after** the volunteers have finished their classifications. To do that, we prepend our metadata names with a `!` character.\n",
    "\n",
    "Fortunately, _Adam McMaster_ has already done the hard work of finding and associatimg the information with the SuperWASP targets. We just need to use the Python client (also developed by Adam, by the way!) to update the metadata of our subjects. We can load those data from a CSV file to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extraSubjectInfo = pd.read_csv(\n",
    "    os.path.join(dataDirectory, \"extendedManifest.csv\")\n",
    ").set_index(\"Subject\")\n",
    "extraSubjectInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting a list of subject sets for our project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine that we haven't already got a reference to our subject sets. Maybe we are updating our project after it's run for a few months. How would we easily get a list of all the subject sets to update? If we know our project's ID number, it's easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = Project.find(projectId)\n",
    "subjectSets = project.links.subject_sets\n",
    "for subjectSet in subjectSets:\n",
    "    print(subjectSet.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over the subjects in all our sets and update their metadata appropriately. Every `SubjectSet` has an attribute `subjects` which points to a list of the `Subject`s it contains. \n",
    "\n",
    "Luckily, all our subjects have an `image` metadatum that we can use to look up the extra metadata in the table we loaded.\n",
    "\n",
    "We just use a bit of Python magic to turn the data in the right row into a `dict` with the table column headings as the keys and the row entries as values. We mustn't forget to add a `!` character to the beginning of the key strings. Once that's done we can use our `dict` to update each `Subject`'s metadata.\n",
    "\n",
    "Don't forget to `save` the subject afterwards!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for subjectSet in subjectSets:\n",
    "    for subject in subjectSet.subjects:\n",
    "        imageId = int(subject.metadata[\"image\"][:-4])\n",
    "        extraData = {\"!\" + k: v for k, v in dict(extraSubjectInfo.loc[imageId]).items()}\n",
    "        subject.metadata.update(extraData)\n",
    "        subject.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check in our web browser to see whether the metadata were added properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Classification and Subject Data\n",
    "The `Project` and `Workflow` classes are both \"exportable\". This means that you can donload a data export in the same way you would from the _Exports_ tab in the project builder.\n",
    "\n",
    "To download a data export we call the `get_export` method on a `Project` or `Workflow` instance. When you call `get_export` you **must** specify an export type as either `\"subjects\"` or `\"classifications\"`. You also have a few other options:\n",
    "\n",
    "* `generate`  - If true then the requested data export will be generated if it does not already exist and regenerated if it does. The default is `False`.\n",
    "* `wait` -  If (re)generation is requested and `wait` is `True`, then your program will wait for the new export to be generated and then download it (remember, for large exports this might take a long time). On the other hand if `wait` is `False` (the default) then a new export will be requested but the program will not wait. You should wait until you get an email telling you that your export is ready, then call `get_export` again with `generate=False`.\n",
    "* `wait_timeout` - the maximum length of time (in seconds) that you are willing to wait for a new export to be generated.\n",
    "\n",
    "Here's how to get the classification and subject exports for our project using Python. In the first case, we know that we already generated a classifications export from the project builder, so we leave `generate` as its default `False` value. In the second case, we specify that we want to generate a new subjects export (because we've updated the metadata) and that we're willing to wait for the export to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificationsResponse = project.get_export(\"classifications\")\n",
    "subjectsResponse = project.get_export(\"subjects\", generate=True, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These calls return Python `Response` objects, which aren't particularly useful without some processing. Luckily, we can use the _Pandas_ package to get thim into a more recognizeable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = pd.read_csv(io.BytesIO(classificationsResponse.content))\n",
    "subjects = pd.read_csv(io.BytesIO(subjectsResponse.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up Feedback and Volunteer Training\n",
    "The Zooniverse platform includes an **experimental** feature that allows you to provide immediate feedback to volunteers after they make a classification. This can be very useful if you want to improve volunteers' confidence when they first join the project, or provide training if your analysis task is somewhat complicated.\n",
    "\n",
    "The fact that feedback is an experimental feature means that you need to submit a request for it to be activated on your project. Send an email to `contact@zooniverse.org` explaining that you would like feedback to be switched on and your reasons form making the request. Once feedback is enabled a new set of options will appear in the task definition section of the project builder.\n",
    "\n",
    "Feedback is available for several of the classification and annotation tools, including:\n",
    "1. Point mark drawing tools - you can specify a circular or elliptical region into which the volunteer's mark must fall in order to be deemed \"correct\". If they place a mark in the region, the system informs them that they scored a \"hit\", otherwise they are informed that they missed, and the correct solution is displayed.\n",
    "2. Question tasks - if a **single** answer is required, you can specify the correct answer and inform the volunteer if they got it right. \n",
    "\n",
    "For our project all our tasks are _single-answer questions_, so we'll demonstrate how to set up that type of feedback. \n",
    "\n",
    "1. The first step in setting up feedback is to specify a set of _task-level_ \"feedback rules\" in the project builder. \n",
    "2. Once you have done that we need some way of specifying what the correct answer for each subject is. Since the correct answer for each task will likely vary from subject to subject, the we use _hidden_ metadata on specific subjects to specify it.\n",
    "3. If the feedback system doesn't find any relevant hidden metadata then no feedback is displayed - the subject is treated as normal.\n",
    "\n",
    "So how do we specify those hidden metadata? Well, all metadata associated with the feeback system have names starting with `#feedback_`. The next part of the name (before a final suffix) is an integer like `1`, or `2`, so our metadatum name might become `#feedback_1` or `#feedback_2`. The number allows you to associate several different feedback _rules_ with the same subject. When you set up rules in the project builder, you assign them all a unique ID number. The number in the metadatum name lets you map the information in that metadatum to a particular rule. However, **be careful**! The number in the metadatum name is not _neccessarily_ the rule ID number. We link a metadata name number with a rule ID using the `_id` suffix for the metadatum name. For example if we want to link all metadata for a particular subject with the number `2` in their **name** to the **rule with ID** `3`, we would add a metadatum to that subject like this:\n",
    "```python\n",
    "subject.metadata.update({ \"#feedback_2_id\" : \"3\" })\n",
    "```\n",
    "Yes, it's a bit complicated but it does provide extra flexibility to the system so that you can have different feedback for different subjects and tasks in a workflow. Now, what other metadata do we need to specify? \n",
    "\n",
    "For the single answer question task, we need to specify the correct answer for each subject too! We do that using the `_answer` suffix. The value of this metadatum corresponds to the position of the correct answer in the list of options you specified in the project builder, **starting from zero**. So for example, if the possible answers were:\n",
    "* Good\n",
    "* Bad\n",
    "* Ugly\n",
    "\n",
    "and the correct answer was \"Ugly\" (at position `2`), then we would add a metadatum to our subject like this:\n",
    "```python\n",
    "subject.metadata.update({ \"#feedback_2_answer\" : 2 })\n",
    "```\n",
    "Remember the `2` in `\"#feedback_2_answer\"` refers to the rule-to-metadatum mapping, while the value `2` refers to the position of the correct answer.\n",
    "\n",
    "Finally we also have an option to add subject-specific messages that will be shown to the volunteer. If you don't specify a subject-specific message, then the workflow/task generic message that must be specified in the project builder will be displayed. \n",
    "\n",
    "Let's assume that we knew a particular subject was really difficult and we wanted to especially congratulate (or comisserate) volunteers who got the right answer (or the wrong answer). We can do that using the `_successMessage` and `failureMessage` suffixes. For our example we would add the following metadata.\n",
    "\n",
    "```python\n",
    "subject.metadata.update({ \n",
    "    \"#feedback_2_successMessage\" : \"Well done! That one was tricky!\" ,\n",
    "    \"#feedback_2_failureMessage\" : \"Hard luck. That one was very difficult!\"\n",
    "})\n",
    "```\n",
    "\n",
    "Okay, let's add feedback metadata to some of the subjects in our project. We'll assume that we've already inspected the images, so we know that correct answers for a set of images. We'll also assume that we identified a couple of difficults ones too. We'll read those data in from a file using the _Pandas_ package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correctAnswers = pd.read_csv(\n",
    "    os.path.join(dataDirectory, \"demoset_0-10/correctAnswers.csv\")\n",
    ").set_index(\"image\")\n",
    "correctAnswers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can loop over all our subjects again and add feedback to the ones we know the answer for (as it happens the're all from the set we uploaded in the project builder!). For each subject, we'll check whether we have an answer and if we do we'll create some new metadata and add it to our subject."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFeedbackMetadata(answer, rule=1, number=1):\n",
    "    feedbackMetadata = {\n",
    "        f\"#feedback_{number}_id\": f\"{rule}\",\n",
    "        f\"#feedback_{number}_answer\": f\"{answer.category_position}\",\n",
    "    }\n",
    "    if answer.difficult:\n",
    "        feedbackMetadata.update(\n",
    "            {\n",
    "                f\"#feedback_{number}_successMessage\": \"Well done! That one was tricky!\",\n",
    "                f\"#feedback_{number}_failureMessage\": \"Hard luck. That was a tricky one\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return feedbackMetadata\n",
    "\n",
    "\n",
    "for subjectSet in subjectSets:\n",
    "    for subject in subjectSet.subjects:\n",
    "        imageId = int(subject.metadata[\"image\"][:-4])\n",
    "        if imageId in correctAnswers.index:\n",
    "            subject.metadata.update(makeFeedbackMetadata(correctAnswers.loc[imageId]))\n",
    "        subject.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tapering feedback frequency\n",
    "Your volunteers will probably benefit more from feedback when they first join the project and after a while, it might just become annoying!\n",
    "\n",
    "The Zooniverse plaform allows you to control how frequently subjects with feedback are shown to volunteers, depending on how many classifications they have performed. Note that because the system needs to know how many subjects a volunteer has classified, it **only works for users who have a Zooniverse account and have logged in**.\n",
    "\n",
    "Technically, this works by controlling the how frequently subjects are selected from **different subject sets**, so you would achieve a tapered feedback by putting all your subjects with feedback in their own subject set and then tapering the frequency with which subjects are drawn from that set.\n",
    "\n",
    "Luckily, we have two subject sets and we just added feedback metadata to one of them, so let's see how we'd implement tapered feedback. We do this by setting some attributes of the _Workflow_ configuration.\n",
    "\n",
    "Zooniverse workflows are represented in Python as instances of the `Workflow` class. We can get a reference to the workflow for our project like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = project.links.workflows[0]\n",
    "print(workflow.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have access to our workflow in Python we need to edit its configuration `dict` and add three new entries.\n",
    "\n",
    "1. `training_set_ids` - a Python `list` holding IDs of the subject sets that contain \"training subjects\" i.e. those with feedback metadata.\n",
    "2. `training_chances` - a Python `list` containing values between 0 and 1. Each value specifies the probability that a particular subject that the volunteer sees will be drawn from one of the training sets. The position of the value in the list corresponds to the number of classifications by the volunteer, so if the first value in the list is 1, then the first subject the volunteer sees is guaranteed to be a training subject. If the fifth value in the list is 0.5 then there is only a 50% chance that this will be a training subject.\n",
    "3. `training_default_chance` - Once the `training_chances` list is exhausted, then the probability for selecting subjects from the training sets will assume this value.\n",
    "\n",
    "For this example, we will force the first three subjects to be training subjects, specify a 50% chance for the next three and then default to a probability of zero thereafter. We do that like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.configuration[\"training_set_ids\"] = [\n",
    "    int(sset.id)\n",
    "    for sset in project.links.subject_sets\n",
    "    if sset.display_name == \"Demo Set\"\n",
    "]\n",
    "workflow.configuration[\"training_chances\"] = [1, 1, 1, 0.5, 0.5, 0.5]\n",
    "workflow.configuration[\"training_default_chance\"] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
